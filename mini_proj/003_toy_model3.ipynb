{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ee02ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPModel, CLIPProcessor\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c5675c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_m = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# both go to lin proj to match final emb vec\n",
    "clip_m_v = clip_m.vision_model\n",
    "clip_m_v_proj = clip_m.visual_projection\n",
    "clip_m_t = clip_m.text_model\n",
    "clip_m_t_proj = clip_m.text_projection\n",
    "\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e66d76b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2457, 0.1930]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# load image\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "img = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# convert image to emb\n",
    "inputs_v = processor(images=img, return_tensors=\"pt\").pixel_values\n",
    "outs_v1 = clip_m_v(inputs_v).pooler_output\n",
    "outs_v_fin = clip_m_v_proj(outs_v1)\n",
    "\n",
    "# convert image to emb\n",
    "inputs_t = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], return_tensors=\"pt\", padding=True)\n",
    "outs_t1 = clip_m_t(**inputs_t).pooler_output\n",
    "outs_t_fin = clip_m_t_proj(outs_t1)\n",
    "\n",
    "# # cosine similarity of image-text\n",
    "# print((outs_v_fin @ outs_t_fin.T))\n",
    "\n",
    "# # Get normalized embeddings (already normalized by CLIP)\n",
    "# image_embeds = outputs.image_embeds  # (batch_size, embedding_dim)\n",
    "# text_embeds = outputs.text_embeds    # (batch_size, embedding_dim)\n",
    "\n",
    "# Normalize embeddings\n",
    "image_embeds_norm = F.normalize(outs_v_fin, p=2, dim=-1)  # Normalize each vector to length 1\n",
    "text_embeds_norm = F.normalize(outs_t_fin, p=2, dim=-1)\n",
    "\n",
    "# # Cosine similarity matrix:\n",
    "# cosine_sim = torch.matmul(image_embeds_norm, text_embeds_norm.T)\n",
    "cosine_sim = image_embeds_norm @ text_embeds_norm.T\n",
    "\n",
    "print(cosine_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b1d3cbb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the image-text similarity score tensor([[24.5701, 19.3049]], grad_fn=<TBackward0>)\n",
      "cosine similarity score tensor([[0.2457, 0.1930]], grad_fn=<MmBackward0>)\n",
      "tensor([[0.9949, 0.0051]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "inputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], images=img, return_tensors=\"pt\", padding=True)\n",
    "outputs = clip_m(**inputs)\n",
    "outputs.keys()\n",
    "# ['logits_per_image', 'logits_per_text', 'text_embeds', 'image_embeds', 'text_model_output', 'vision_model_output']\n",
    "logits_per_image = outputs.logits_per_image # this is the image-text similarity score or just logits, not normalized\n",
    "print(f\"the image-text similarity score {logits_per_image}\")\n",
    "print(f\"cosine similarity score {outputs.image_embeds@ outputs.text_embeds.T}\")\n",
    "probs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a2fbd497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPTokenizerFast(name_or_path='openai/clip-vit-base-patch32', vocab_size=49408, model_max_length=77, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|startoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t49406: AddedToken(\"<|startoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t49407: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "966f4c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS_token_id = processor.tokenizer.convert_tokens_to_ids(\"<|startoftext|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3d15b556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs_v_fin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9e52db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from pathlib import Path\n",
    "\n",
    "#Tokenizer from scratch on vocabulary of corpus\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "# Decoder\n",
    "from transformers import RobertaConfig\n",
    "from transformers import RobertaForMaskedLM # RobertaLM for learning\n",
    "from transformers import RobertaTokenizerFast # After training tokenizer we will wrap it so it can be used by Roberta model\n",
    "\n",
    "#Encoder-Decoder Model\n",
    "from transformers import VisionEncoderDecoderModel\n",
    "\n",
    "#Training\n",
    "# When using previous version of the library you need the following two lines\n",
    "from transformers import Seq2SeqTrainer\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "# Latest version imports\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "import torch\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85c8dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "def train_test_split(dictionary):\n",
    "    images = dictionary.keys()\n",
    "    images_test = random.sample(images,int(0.3*len(images)))\n",
    "    images_train = [img for img in images if img not in images_test]\n",
    "\n",
    "    train_dict = {\n",
    "      img: dictionary[img] for img in images_train\n",
    "    }\n",
    "\n",
    "    test_dict = {\n",
    "      img: dictionary[img] for img in images_test\n",
    "    }\n",
    "    return(train_dict,test_dict)\n",
    "\n",
    "train,test = train_test_split(images_caption_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e99e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_df(dictionary):\n",
    "    df = pd.DataFrame([])\n",
    "\n",
    "    captions = []\n",
    "    images = []\n",
    "    for image in list(images_caption_dict.keys()):\n",
    "        caption= images_caption_dict[image]\n",
    "        if use_all == True:\n",
    "            captions.append(tokenizer.sep_token.join([' '.join(capt.replace('<s> ','').replace('  <e>','').strip().split(' ') for capt in caption])\n",
    "        else:\n",
    "            for capt in caption:\n",
    "                captions.append(' '.join(capt.replace('<s> ','').replace('  <e>','').strip().split(' ')[:30]))\n",
    "                images.append(image)\n",
    "\n",
    "    df['images'] = images\n",
    "    df['captions'] = captions\n",
    "    return(df)\n",
    "\n",
    "train_df = get_df(train)\n",
    "test_df = get_df(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66471289",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTFeatureExtractor\n",
    "from transformers import RobertaTokenizerFast\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('Byte_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9220f20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "projected_image_embedding = projection_layer(image_embedding)  # shape: (batch_size, prefix_length * lm_embed_dim)\n",
    "projected_image_embedding = projected_image_embedding.view(batch_size, prefix_length, lm_embed_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98daaabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagePrefixCaptioner(nn.Module):\n",
    "    def __init__(self, image_encoder, lm_model, prefix_length, embed_dim):\n",
    "        super().__init__()\n",
    "        self.image_encoder = image_encoder\n",
    "        self.lm_model = lm_model\n",
    "        self.prefix_length = prefix_length\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        self.projection = nn.Linear(image_encoder.output_dim, prefix_length * embed_dim)\n",
    "    \n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        batch_size = images.size(0)\n",
    "        image_embeds = self.image_encoder(images)  # (batch_size, image_embed_dim)\n",
    "        prefix_embeds = self.projection(image_embeds).view(batch_size, self.prefix_length, self.embed_dim)\n",
    "        \n",
    "        token_embeds = self.lm_model.transformer.wte(input_ids)\n",
    "        \n",
    "        inputs_embeds = torch.cat([prefix_embeds, token_embeds], dim=1)\n",
    "        \n",
    "        prefix_attention_mask = torch.ones(batch_size, self.prefix_length).to(input_ids.device)\n",
    "        extended_attention_mask = torch.cat([prefix_attention_mask, attention_mask], dim=1)\n",
    "        \n",
    "        outputs = self.lm_model(inputs_embeds=inputs_embeds, attention_mask=extended_attention_mask, labels=input_ids)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "78dc9889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs_v_fin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6c8b4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "954cc04e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[49406,   320,  1125,   539,   320,  2368, 49407]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3110c05a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None <|endoftext|> <|endoftext|> None 50256 50256\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "dec_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "dec_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "print(\n",
    "    dec_tokenizer.pad_token, dec_tokenizer.eos_token, dec_tokenizer.bos_token, \n",
    "    dec_tokenizer.pad_token_id, dec_tokenizer.eos_token_id, dec_tokenizer.bos_token_id,\n",
    "    )\n",
    "\n",
    "dec_tokenizer.pad_token = dec_tokenizer.eos_token\n",
    "dec_tokenizer.pad_token_id = dec_tokenizer.eos_token_id\n",
    "\n",
    "# Confirm embedding size\n",
    "print(dec_model.config.n_embd)  # 768 for GPT2-base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dba6a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 32])\n",
      "torch.Size([1, 7, 32])\n",
      "final emb size from encoder torch.Size([1, 23, 768])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected target size [1, 50257], got [1, 22]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 71\u001b[0m\n\u001b[1;32m     66\u001b[0m logits\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;66;03m# torch.Size([1, 23, 50257])\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# loss = outputs.loss\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# loss.backward()\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# # optimizer step, etc.\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(logits[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :], labels[:, \u001b[38;5;241m1\u001b[39m:])\n",
      "File \u001b[0;32m~/work/anaconda3/envs/mlx802/lib/python3.11/site-packages/torch/nn/functional.py:3086\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3084\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3085\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3086\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mcross_entropy_loss(\u001b[38;5;28minput\u001b[39m, target, weight, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected target size [1, 50257], got [1, 22]"
     ]
    }
   ],
   "source": [
    "# Get vision embedding from image encoder\n",
    "\n",
    "max_length = 512\n",
    "model_d = 512\n",
    "batch_size = 1\n",
    "prefix_length = 16\n",
    "lm_embed_v_dim = int(model_d//prefix_length)\n",
    "target_m_dim = 768\n",
    "\n",
    "text = [\"a photo of a cat\"]\n",
    "\n",
    "lin_m_v = nn.Linear(model_d, model_d)\n",
    "lin_m_t = nn.Linear(model_d, lm_embed_v_dim)\n",
    "lin_m_vt4dec = nn.Linear(lm_embed_v_dim, target_m_dim)\n",
    "\n",
    "# load image\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "img = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# convert image to emb\n",
    "inputs_v = processor(images=img, return_tensors=\"pt\").pixel_values\n",
    "outs_v1 = clip_m_v(inputs_v).pooler_output\n",
    "outs_v_fin = clip_m_v_proj(outs_v1)\n",
    "\n",
    "# projection layer\n",
    "projected_image_embed = lin_m_v(outs_v_fin)  # shape: (batch_size, prefix_length * lm_embed_dim)\n",
    "projected_image_embed = projected_image_embed.view(batch_size, prefix_length, lm_embed_v_dim)\n",
    "print(projected_image_embed.shape) # (batch_size, prefix_length, lm_embed_dim)\n",
    "\n",
    "# convert text to emb\n",
    "inputs_t = processor(text=text, return_tensors=\"pt\", padding=True)\n",
    "outs_t1 = clip_m_t(**inputs_t).last_hidden_state\n",
    "outs_t_fin = clip_m_t_proj(outs_t1) # shape: (batch_size, seq_len, lm_embed_dim)\n",
    "projected_text_embed = lin_m_t(outs_t_fin)  # shape: (batch_size, prefix_length * lm_embed_dim)\n",
    "print(projected_text_embed.shape)\n",
    "# outs_t_fin.shape # torch.Size([1, 7, 512])\n",
    "\n",
    "# Concatenate image prefix embeddings with token embeddings\n",
    "inputs_embeds = torch.cat([projected_image_embed, projected_text_embed], dim=1)\n",
    "# inputs_embeds.shape # [batch_size, img_seq+text_seq, model_d_sub]\n",
    "inputs_embeds4dec_m = lin_m_vt4dec(inputs_embeds)\n",
    "print(f\"final emb size from encoder {inputs_embeds4dec_m.shape}\") # torch.Size([1, 23, 768]) or [batch, img+txt, dec_dim]\n",
    "\n",
    "# Modify attention mask to accommodate prefix tokens\n",
    "prefix_attention_mask = torch.ones(batch_size, prefix_length) #.to(device)\n",
    "attention_mask = torch.cat([prefix_attention_mask, inputs_t.attention_mask], dim=1)\n",
    "\n",
    "# set label\n",
    "# Prepare labels\n",
    "text_with_eos = [dec_tokenizer.eos_token + t + dec_tokenizer.eos_token for t in text]  # '<|endoftext|>'\n",
    "labels = dec_tokenizer(text_with_eos, return_tensors=\"pt\", padding=True).input_ids\n",
    "# labels = dec_tokenizer(text, return_tensors=\"pt\", padding=True).input_ids\n",
    "\n",
    "# Pad labels to match full input length (prefix + text)\n",
    "# Set prefix part to -100 so loss is ignored there\n",
    "padding_labels = torch.full((batch_size, prefix_length), -100)  # ignore loss here\n",
    "labels = torch.cat([padding_labels, labels], dim=1)  # (batch_size, prefix_len + text_len)\n",
    "\n",
    "out_dec = dec_model(\n",
    "    inputs_embeds=inputs_embeds4dec_m, \n",
    "    attention_mask=attention_mask, \n",
    "    labels=labels,  # this triggers loss computation\n",
    "    )\n",
    "# out_dec.keys() # ['loss', 'logits', 'past_key_values']\n",
    "logits = out_dec.logits\n",
    "logits.shape # torch.Size([1, 23, 50257])\n",
    "# loss = outputs.loss\n",
    "# loss.backward()\n",
    "# # optimizer step, etc.\n",
    "\n",
    "# Shifted for causal LM (already done internally if you pass labels to the model directly)\n",
    "loss = F.cross_entropy(\n",
    "    logits.view(-1, logits.size(-1)),  # [batch_size * seq_len, vocab_size]\n",
    "    labels.view(-1),                   # [batch_size * seq_len]\n",
    "    ignore_index=-100                 # to ignore prefix tokens or padding\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f4ee83c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([23, 50257])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.view(-1, logits.size(-1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b879d233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 23, 50257])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dfd98aef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.5349, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = out_dec.loss\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "33dfc89d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.8355, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flatten\n",
    "logits_flat = logits.view(-1, logits.size(-1))       # [batch * seq_len, vocab_size]\n",
    "labels_flat = labels.view(-1)                        # [batch * seq_len]\n",
    "\n",
    "loss = F.cross_entropy(logits_flat, labels_flat, ignore_index=-100)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b976cb00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.5349, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Shift logits to exclude the last prediction\n",
    "logits = out_dec.logits[:, :-1, :]   # Predict tokens at t+1\n",
    "\n",
    "# Step 2: Shift labels to exclude the first token\n",
    "labels = labels[:, 1:]               # Targets for prediction\n",
    "\n",
    "# Step 3: Flatten for cross-entropy\n",
    "logits = logits.reshape(-1, logits.size(-1))  # (batch * seq_len-1, vocab_size)\n",
    "labels = labels.reshape(-1)                   # (batch * seq_len-1)\n",
    "\n",
    "# Step 4: Apply loss (ignore -100 label positions, e.g., for image prefix or padding)\n",
    "loss = F.cross_entropy(logits, labels, ignore_index=-100)\n",
    "loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx802",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
